#!/usr/bin/env python3
"""
Gmail Digest â€” å®Œæ•´é‚®ä»¶å¤„ç†ç®¡é“
1. è½®è¯¢æ–°é‚®ä»¶ (gog gmail search)
2. è¯»å–é‚®ä»¶å…¨æ–‡ (gog gmail thread)
3. æå–å¹¶æŠ“å–æ–‡ç« é“¾æ¥
4. è°ƒç”¨ Gemini API ç”Ÿæˆä¸­æ–‡æ‘˜è¦
5. é€šè¿‡ TG Bot å‘é€æ‘˜è¦
"""

import json, os, sys, re, subprocess, time, logging, base64
from urllib.parse import urlparse
import requests
import html2text

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ACCOUNT      = os.environ.get('GMAIL_ACCOUNT', 'klchen0113@gmail.com')
STATE_FILE   = os.environ.get('STATE_FILE', '/var/lib/clawdbot/gmail-poll-state.txt')
BOT_TOKEN    = os.environ.get('BOT_TOKEN', '')
CHAT_ID      = os.environ.get('CHAT_ID', '6309937609')
GEMINI_KEY   = os.environ.get('GOOGLE_API_KEY', '')
DEEPSEEK_KEY = os.environ.get('DEEPSEEK_API_KEY', '')
MAX_URLS     = 3           # æ¯å°é‚®ä»¶æœ€å¤šæŠ“å– 3 ä¸ªé“¾æ¥
URL_TIMEOUT  = 10          # æŠ“å–é“¾æ¥è¶…æ—¶ (ç§’)
MAX_ARTICLE  = 3000        # æ¯ç¯‡æ–‡ç« æœ€å¤šä¿ç•™å­—ç¬¦æ•°
MAX_BODY     = 5000        # é‚®ä»¶æ­£æ–‡æœ€å¤šä¿ç•™å­—ç¬¦æ•°
SEARCH_QUERY = 'is:unread newer_than:10m'

# AI API é…ç½®
GEMINI_MODEL = 'gemini-2.0-flash'
GEMINI_URL   = f'https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent'
DEEPSEEK_URL = 'https://api.deepseek.com/v1/chat/completions'

# â”€â”€ æ—¥å¿— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [gmail-digest] %(levelname)s %(message)s',
    datefmt='%H:%M:%S'
)
log = logging.getLogger('gmail-digest')

# â”€â”€ è·³è¿‡çš„åŸŸå (å¹¿å‘Š/è¿½è¸ª/é€€è®¢) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SKIP_DOMAINS = {
    'list-manage.com', 'mailchimp.com', 'sendgrid.net',
    'manage.kmail-lists.com', 'google.com/maps',
    'play.google.com', 'itunes.apple.com',
    'facebook.com', 'twitter.com', 'instagram.com',
    'linkedin.com', 'youtube.com',
    'doubleclick.net', 'googlesyndication.com',
}

SKIP_URL_PATTERNS = [
    'unsubscribe', 'optout', 'opt-out', 'preference',
    'click.', 'tracking.', 'trk.', 'opens.',
    'beacon', 'pixel', '1x1',
]

h2t = html2text.HTML2Text()
h2t.ignore_links = False
h2t.ignore_images = True
h2t.body_width = 0
h2t.ignore_emphasis = True


def run_gog(args, timeout=30):
    """æ‰§è¡Œ gog å‘½ä»¤å¹¶è¿”å› stdout"""
    env = os.environ.copy()
    env['GOG_KEYRING_BACKEND'] = 'file'
    env['GOG_KEYRING_PASSWORD'] = os.environ.get('GOG_KEYRING_PASSWORD', 'gogpass')
    result = subprocess.run(
        ['gog'] + args,
        capture_output=True, text=True, timeout=timeout, env=env
    )
    if result.returncode != 0:
        log.warning('gog error: %s', result.stderr[:200])
    return result.stdout


def mark_as_read(thread_id):
    """æ ‡è®°é‚®ä»¶ä¸ºå·²è¯» (ç§»é™¤ UNREAD æ ‡ç­¾)"""
    out = run_gog([
        'gmail', 'thread', 'modify', thread_id,
        '--remove', 'UNREAD',
        '--account', ACCOUNT,
        '--force',
    ])
    if out is not None:
        log.info('Marked as read: %s', thread_id[:20])
    return True


def load_seen_ids():
    """åŠ è½½å·²å¤„ç†çš„é‚®ä»¶ ID"""
    if not os.path.exists(STATE_FILE):
        return set()
    with open(STATE_FILE) as f:
        return {line.strip() for line in f if line.strip()}


def save_seen_ids(ids):
    """ä¿å­˜å·²å¤„ç†çš„é‚®ä»¶ ID (ä¿ç•™æœ€è¿‘ 500 ä¸ª)"""
    recent = sorted(ids)[-500:]
    os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
    with open(STATE_FILE, 'w') as f:
        f.write('\n'.join(recent) + '\n')


def search_new_emails():
    """æœç´¢æ–°æœªè¯»é‚®ä»¶"""
    raw = run_gog([
        'gmail', 'search', SEARCH_QUERY,
        '--account', ACCOUNT,
        '--json', '--results-only'
    ])
    try:
        return json.loads(raw) if raw.strip() else []
    except json.JSONDecodeError:
        log.error('Failed to parse search results: %s', raw[:200])
        return []


def get_email_body(thread_id):
    """è¯»å–é‚®ä»¶å…¨æ–‡ï¼Œè¿”å› {from, subject, date, body, snippet}"""
    raw = run_gog([
        'gmail', 'thread', thread_id,
        '--account', ACCOUNT,
        '--json', '--results-only'
    ])
    try:
        data = json.loads(raw)
    except (json.JSONDecodeError, TypeError):
        log.error('Failed to parse thread %s', thread_id)
        return {}

    thread = data.get('thread', {})
    messages = thread.get('messages', [])
    if not messages:
        return {}

    msg = messages[0]
    payload = msg.get('payload', {})
    snippet = msg.get('snippet', '')

    # æå– headers
    headers = {}
    for h in payload.get('headers', []):
        name = h.get('name', '').lower()
        if name in ('from', 'to', 'subject', 'date'):
            headers[name] = h.get('value', '')

    # æå– body
    body_text = ''
    body_html = ''

    def extract_parts(parts):
        nonlocal body_text, body_html
        for part in parts:
            mime = part.get('mimeType', '')
            body_data = part.get('body', {}).get('data', '')
            if body_data:
                decoded = base64.urlsafe_b64decode(body_data).decode('utf-8', errors='replace')
                if mime == 'text/plain' and not body_text:
                    body_text = decoded
                elif mime == 'text/html' and not body_html:
                    body_html = decoded
            if 'parts' in part:
                extract_parts(part['parts'])

    if 'parts' in payload:
        extract_parts(payload['parts'])
    else:
        body_data = payload.get('body', {}).get('data', '')
        if body_data:
            decoded = base64.urlsafe_b64decode(body_data).decode('utf-8', errors='replace')
            mime = payload.get('mimeType', '')
            if mime == 'text/plain':
                body_text = decoded
            elif mime == 'text/html':
                body_html = decoded

    # å¦‚æœåªæœ‰ HTMLï¼Œè½¬æ¢ä¸ºæ–‡æœ¬
    if not body_text and body_html:
        body_text = h2t.handle(body_html)

    return {
        'from': headers.get('from', ''),
        'subject': headers.get('subject', ''),
        'date': headers.get('date', ''),
        'body': body_text[:MAX_BODY],
        'snippet': snippet,
    }


def is_tracking_url(url):
    """åˆ¤æ–­æ˜¯å¦ä¸ºè¿½è¸ª/å¹¿å‘Šé“¾æ¥"""
    url_lower = url.lower()
    for pat in SKIP_URL_PATTERNS:
        if pat in url_lower:
            return True
    parsed = urlparse(url_lower)
    domain = parsed.netloc
    for sd in SKIP_DOMAINS:
        if sd in domain:
            return True
    return False


def extract_article_urls(body_text):
    """ä»é‚®ä»¶æ­£æ–‡æå–æœ‰ä»·å€¼çš„æ–‡ç« é“¾æ¥"""
    urls = re.findall(r'https?://[^\s<>"\')\]]+', body_text)

    seen = set()
    good_urls = []
    for url in urls:
        url = url.rstrip('.,;:!?)>]')
        if len(url) < 20:
            continue
        if is_tracking_url(url):
            continue

        # è·³è¿‡å›¾ç‰‡/åª’ä½“
        path_lower = urlparse(url).path.lower()
        if any(path_lower.endswith(ext) for ext in
               ('.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico', '.css', '.js', '.woff')):
            continue

        # å»é‡
        parsed = urlparse(url)
        key = parsed.netloc + parsed.path
        if key in seen:
            continue
        seen.add(key)
        good_urls.append(url)

    return good_urls[:MAX_URLS]


def fetch_medium_article(url):
    """ç”¨ Playwright session æŠ“å– Medium ä»˜è´¹æ–‡ç« """
    try:
        result = subprocess.run(
            ['python3', '/opt/clawdbot/medium-fetch.py', url],
            capture_output=True, text=True, timeout=30
        )
        if result.returncode == 0 and len(result.stdout.strip()) > 50:
            return result.stdout.strip()[:MAX_ARTICLE]
    except Exception as e:
        log.warning('Medium fetch failed: %s', e)
    return None


def fetch_article(url):
    """æŠ“å–æ–‡ç« å†…å®¹ï¼Œè¿”å›çº¯æ–‡æœ¬ (Medium ç”¨ Playwrightï¼Œå…¶ä»–ç”¨ requests)"""
    if 'medium.com' in url:
        content = fetch_medium_article(url)
        if content:
            return content

    try:
        resp = requests.get(url, timeout=URL_TIMEOUT, headers={
            'User-Agent': 'Mozilla/5.0 (compatible; ClawdBot/1.0)',
            'Accept': 'text/html,application/xhtml+xml',
        }, allow_redirects=True)
        resp.raise_for_status()

        content_type = resp.headers.get('Content-Type', '')
        if 'html' not in content_type and 'text' not in content_type:
            return None

        text = h2t.handle(resp.text)
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = text.strip()

        if len(text) < 50:
            return None

        return text[:MAX_ARTICLE]
    except Exception as e:
        log.warning('Failed to fetch %s: %s', url[:60], e)
        return None


def call_deepseek(prompt):
    """è°ƒç”¨ DeepSeek API ç”Ÿæˆæ‘˜è¦"""
    if not DEEPSEEK_KEY:
        return None
    try:
        resp = requests.post(
            DEEPSEEK_URL,
            headers={'Authorization': f'Bearer {DEEPSEEK_KEY}', 'Content-Type': 'application/json'},
            json={
                'model': 'deepseek-chat',
                'messages': [{'role': 'user', 'content': prompt}],
                'temperature': 0.3,
                'max_tokens': 1024,
            },
            timeout=60,
        )
        resp.raise_for_status()
        data = resp.json()
        choices = data.get('choices', [])
        if choices:
            return choices[0].get('message', {}).get('content', '')
        log.warning('DeepSeek returned empty')
        return None
    except Exception as e:
        log.error('DeepSeek API error: %s', e)
        return None


def call_gemini(prompt):
    """è°ƒç”¨ Gemini API ç”Ÿæˆæ‘˜è¦"""
    if not GEMINI_KEY:
        return None
    try:
        resp = requests.post(
            GEMINI_URL,
            params={'key': GEMINI_KEY},
            json={
                'contents': [{'parts': [{'text': prompt}]}],
                'generationConfig': {
                    'temperature': 0.3,
                    'maxOutputTokens': 1024,
                }
            },
            timeout=30,
        )
        resp.raise_for_status()
        data = resp.json()
        candidates = data.get('candidates', [])
        if candidates:
            parts = candidates[0].get('content', {}).get('parts', [])
            if parts:
                return parts[0].get('text', '')
        log.warning('Gemini returned empty: %s', json.dumps(data)[:200])
        return None
    except Exception as e:
        log.error('Gemini API error: %s', e)
        return None


def call_ai(prompt):
    """è°ƒç”¨ AI API (ä¼˜å…ˆ Geminiï¼Œå¤±è´¥åˆ™å›é€€ DeepSeek)"""
    result = call_gemini(prompt)
    if result:
        log.info('AI summary via Gemini')
        return result
    result = call_deepseek(prompt)
    if result:
        log.info('AI summary via DeepSeek')
        return result
    log.warning('All AI APIs failed')
    return None


def build_summary_prompt(email_data, articles):
    """æ„å»º AI æ‘˜è¦æç¤º"""
    prompt = f"""è¯·ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹é‚®ä»¶ï¼Œè¦æ±‚ï¼š
1. ä¸€å¥è¯æ¦‚æ‹¬é‚®ä»¶ä¸»æ—¨
2. åˆ—å‡ºå…³é”®ä¿¡æ¯è¦ç‚¹ (3-5æ¡)
3. å¦‚æœæœ‰æ–‡ç« é“¾æ¥å†…å®¹ï¼Œæå–æ ¸å¿ƒè§‚ç‚¹
4. å¦‚æœéœ€è¦é‡‡å–è¡ŒåŠ¨ï¼Œæ˜ç¡®æ ‡æ³¨

ğŸ“§ é‚®ä»¶ä¿¡æ¯
å‘ä»¶äºº: {email_data.get('from', '?')}
ä¸»é¢˜: {email_data.get('subject', '?')}
æ—¥æœŸ: {email_data.get('date', '?')}

ğŸ“ é‚®ä»¶æ­£æ–‡:
{email_data.get('body', '')[:3000]}"""

    if articles:
        prompt += '\n\nğŸ“ é“¾æ¥æ–‡ç« å†…å®¹:'
        for i, a in enumerate(articles, 1):
            prompt += f'\n\n[æ–‡ç« {i}] {a["url"][:80]}\n{a["content"][:1500]}'

    return prompt


def send_tg(text):
    """å‘é€æ¶ˆæ¯åˆ° TG"""
    if not BOT_TOKEN:
        log.warning('BOT_TOKEN not set')
        return False

    if len(text) > 4000:
        text = text[:3997] + '...'

    try:
        resp = requests.post(
            f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage',
            json={'chat_id': CHAT_ID, 'text': text, 'parse_mode': 'HTML',
                  'disable_web_page_preview': True},
            timeout=10
        )
        if resp.status_code == 200:
            return True
        # å¦‚æœ HTML è§£æå¤±è´¥ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬
        resp2 = requests.post(
            f'https://api.telegram.org/bot{BOT_TOKEN}/sendMessage',
            json={'chat_id': CHAT_ID, 'text': text,
                  'disable_web_page_preview': True},
            timeout=10
        )
        return resp2.status_code == 200
    except Exception as e:
        log.error('TG send failed: %s', e)
        return False


def process_email(email_meta, seen_ids):
    """å¤„ç†å•å°é‚®ä»¶: è¯»å– â†’ æå–é“¾æ¥ â†’ æŠ“å–æ–‡ç«  â†’ AIæ€»ç»“ â†’ TGå‘é€"""
    msg_id = email_meta.get('id', '')
    if msg_id in seen_ids:
        return False

    subject = email_meta.get('subject', '(æ— ä¸»é¢˜)')
    log.info('Processing: %s', subject[:60])

    # 1. è¯»å–é‚®ä»¶å…¨æ–‡
    email_data = get_email_body(msg_id)
    if not email_data:
        log.warning('Could not read email %s', msg_id)
        seen_ids.add(msg_id)
        return False

    email_data['id'] = msg_id

    # 2. æå–æ–‡ç« é“¾æ¥
    body = email_data.get('body', '')
    urls = extract_article_urls(body)
    log.info('Found %d article URLs', len(urls))

    # 3. æŠ“å–æ–‡ç« å†…å®¹
    articles = []
    for url in urls:
        content = fetch_article(url)
        if content:
            articles.append({'url': url, 'content': content})
            log.info('Fetched: %s (%d chars)', url[:60], len(content))

    # 4. è°ƒç”¨ AI ç”Ÿæˆæ·±åº¦æ‘˜è¦
    prompt = build_summary_prompt(email_data, articles)
    summary = call_ai(prompt)

    # 5. å‘é€åˆ° TG
    if summary:
        tg_text = f"ğŸ“§ <b>{email_data.get('subject', '?')}</b>\n"
        tg_text += f"ğŸ‘¤ {email_data.get('from', '?')}\n\n"
        tg_text += summary
        log.info('AI summary generated (%d chars)', len(summary))
    else:
        # Gemini ä¸å¯ç”¨æ—¶ï¼Œå‘é€åŸæ–‡æ‘˜è¦
        tg_text = f"ğŸ“§ æ–°é‚®ä»¶\nå‘ä»¶äºº: {email_data.get('from', '?')}\n"
        tg_text += f"ä¸»é¢˜: {email_data.get('subject', '?')}\n\n"
        tg_text += email_data.get('snippet', '')[:500]
        log.info('Using raw snippet (no AI)')

    ok = send_tg(tg_text)
    if ok:
        log.info('Sent to TG: %s', subject[:40])
        # 6. æ ‡è®°ä¸ºå·²è¯»
        mark_as_read(msg_id)
    else:
        log.error('Failed to send to TG: %s', subject[:40])

    seen_ids.add(msg_id)
    return True


def main():
    log.info('Starting gmail-digest for %s', ACCOUNT)

    if not GEMINI_KEY and not DEEPSEEK_KEY:
        log.warning('No AI API key set, summaries disabled')
    if not BOT_TOKEN:
        log.error('BOT_TOKEN not set, cannot send to TG')
        sys.exit(1)

    seen_ids = load_seen_ids()
    log.info('Loaded %d seen IDs', len(seen_ids))

    emails = search_new_emails()
    log.info('Found %d recent emails', len(emails))

    new_count = 0
    for email_meta in emails:
        if process_email(email_meta, seen_ids):
            new_count += 1
            time.sleep(1)

    save_seen_ids(seen_ids)
    log.info('Done: %d new emails processed', new_count)


if __name__ == '__main__':
    main()
